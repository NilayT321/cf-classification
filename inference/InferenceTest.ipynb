{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb29ae49-d0fa-41df-a11c-7a14bb502772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abca9d-466a-4b37-9ee9-74c0d377d19d",
   "metadata": {},
   "source": [
    "We load our model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbe5491-dcce-4cf1-9368-67db1e244afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "\n",
    "class TagModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TagModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('prajjwal1/bert-small')\n",
    "        self.dropout = nn.Dropout(0.4)  # Experiment with dropout rate (e.g., 0.1-0.5)\n",
    "        self.tag_classifier = nn.Sequential(\n",
    "            nn.Linear(512, n),  # Your original linear layer\n",
    "            nn.LayerNorm(n)   # Layer Normalization layer\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)  # Apply dropout\n",
    "        tag_pred = self.tag_classifier(pooled_output)\n",
    "        return tag_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa93a54-2797-42ca-9977-28a6aace0d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TagModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 512)\n",
       "      (token_type_embeddings): Embedding(2, 512)\n",
       "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.4, inplace=False)\n",
       "  (tag_classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=25, bias=True)\n",
       "    (1): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TagModel()\n",
    "model.load_state_dict(torch.load(\"../models/exp_LR_OldData.pth\", map_location = torch.device('cpu'), weights_only = True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c1687a-ad05-48d6-ad30-6233fe9a6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statement = \"\"\"\n",
    "You are given two strings s and t. In one operation, you can delete all the odd-indexed characters from s or all the even-indexed characters from s\n",
    "\n",
    ".\n",
    "\n",
    "For example, if you perform an operation on the string abcdefg, you could choose to turn it into aceg or bdf.\n",
    "\n",
    "After performing any number of operations on s\n",
    "(including zero), is it possible for s to equal t?\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizer stuff\n",
    "MAXLEN = 512\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def tokenize_text(text):\n",
    "  return tokenizer(text, padding='max_length', truncation=True, max_length=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82fac3e-5391-44a3-a15a-e0cf0f161cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: write a function that runs the model on the validation data set, and determines its accuracy on each specific tag. Returns a vector length n with the % accuracy for each tag. The model uses a threshold of 0.5 for classification. remember to apply sigmoid to map the output layer to [0,1]\n",
    "\n",
    "def evaluate_tag_accuracy(model, val_loader, device, n):\n",
    "  \"\"\"\n",
    "  Evaluates the model's accuracy on the validation dataset for each tag.\n",
    "\n",
    "  Args:\n",
    "    model: The trained model.\n",
    "    val_loader: The DataLoader for the validation dataset.\n",
    "    device: The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "    n: The number of tags.\n",
    "\n",
    "  Returns:\n",
    "    A list of floats, representing the accuracy for each tag.\n",
    "  \"\"\"\n",
    "\n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  tag_correct_counts = [0] * n\n",
    "  tag_total_counts = [0] * n\n",
    "  avg_loss = 0\n",
    "  with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "      input_ids, attention_mask, true_tags = batch\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "      true_tags = true_tags.to(device)\n",
    "\n",
    "      tag_pred = model(input_ids, attention_mask)\n",
    "      loss = nn.BCEWithLogitsLoss()(tag_pred, true_tags)\n",
    "      avg_loss += loss.item()\n",
    "      tag_pred = torch.sigmoid(tag_pred)  # Apply sigmoid to get probabilities in [0,1]\n",
    "\n",
    "      predicted_tags = (tag_pred > 0.5).float()\n",
    "\n",
    "      for i in range(n):\n",
    "        tag_correct_counts[i] += (predicted_tags[:, i] == true_tags[:, i]).sum().item()\n",
    "        tag_total_counts[i] += true_tags.shape[0]\n",
    "  avg_loss /= len(val_loader)\n",
    "\n",
    "  tag_accuracies = []\n",
    "  for i in range(n):\n",
    "    if tag_total_counts[i] > 0:\n",
    "      tag_accuracies.append(tag_correct_counts[i] / tag_total_counts[i] * 100)\n",
    "    else:\n",
    "      tag_accuracies.append(0.0)\n",
    "\n",
    "  return (avg_loss,tag_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
