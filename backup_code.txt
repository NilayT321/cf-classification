# Find the table
table = page_0_parser.find('table', attrs = {'class' : 'problems'})
table = table.find('tbody') # Get the body of the table 

# First row 
first_row = table.find_all('tr')[21]
cells = first_row.find_all('td')

# First cell contains the index in an a tag 
index = cells[0].find('a').get_text().replace('\\n', '').strip()

# Second cell contains two div tags
# First one has the name in an a tag 
# Second one has all tags, each within an a tag 
second_cell = cells[1]
second_divs = second_cell.find_all('div')

name = second_divs[0].find('a').get_text().replace('\\n', '').strip()

# Get all tags 
# But some won't have tags at the time of scraping. In which case, just skip 
tags = []
if second_divs[1].find_all('a'):
    tags = [clean_output(link.get_text()) for link in second_divs[1].find_all('a')]
tags = ",".join(tags)

# Rating is stored in the fourth cell (ignore third cell)
# It may be empty as well. In which case, we set default to -999
if cells[3].find('span') is None:
    rating = -999 
else:
    rating = int(clean_output(cells[3].find('span').get_text()))

print(index, name, tags, rating)

# Now, get the problem statement. Some problems are in PDF form, so this will fail
# Find the link of the index 
try:
    # Find link of the name and click it  
    driver.find_element(By.LINK_TEXT, name).click()

    # Get the html source of this page 
    prob_page = str(driver.page_source.encode())
    prob_parser = BeautifulSoup(prob_page, "html.parser")

    # Find the div tag of the problem statements. 
    prob_div = prob_parser.find('div', attrs = {'class' : 'problem-statement'})
    prob_div = prob_div.find_all('div')[10]

    print(clean_output(prob_div.get_text()))

    # prob_text = [clean_output(tag.get_text()) for tag in prob_div.find_all()]
    # prob_statement = "\n".join(prob_text)

    # print(prob_statement)
except:
    # Nothing special needs to be done. Just move on
    pass
finally:
    driver.back() # Go back to the main page 
# 